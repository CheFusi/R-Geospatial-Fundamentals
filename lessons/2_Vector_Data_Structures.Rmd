---
title: "2_Vector_Data_Structures"
output: html_document
date: "2023-10-16"
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Learning Objectives

\*\*\* Compare my version to the old and I can then make a critique about how this version actually improves on the current. \*\*\*

We will work to

**2.1 Understand the content of spatial dataframes and the different geometry types**

-   Why: An understanding of the structure of spatial dataframes allows for accurate manipulation and analysis, while recognizing different geometry types allows for nuanced representation of various geographic features.

**2.2 Create thematic maps and understand the information depicted**

-   Thematic maps visually communicate spatial patterns, enabling intuitive interpretation and comparison of spatial patterns and data distributions.

**2.3 Navigate different coordinate reference systems (CRS)**

-   CRS knowledge, including encoding, transformation, and selection, ensures accurate integration of spatial data from various sources.

------------------------------------------------------------------------

### Overview

Some of the questions we hope to answer are

1.  Spatial data structure

    -   What defines a spatial dataframe

    -   Which base R dataframe functions are applicable to spatial dataframes

2.  Basic Plotting

    -   what is the difference between plotting data of, for example, an XY plot versus mapping said data

3.  Coordinate Reference Systems (CRS)

    -   What are different ways to transform earths shape and size to best suit our analysis

------------------------------------------------------------------------

## **2.X. Geospatial data**

-   encodes data with a location (where) via coordinates, with attributes (what) which is the data found at that location and often with metadata (e.g. ...)

## **2.1. Spatial Data Structure**

[**Objective: Understand the content of spatial dataframes and the different geometry types**]{.underline}

*Intro to section*

-   ...

-   ...

------------------------------------------------------------------------

### A. Defining spatial dataframes

Introduction to sf library.

-   SF = simple features; package designed to simplify analysis of spatial data

```{r}

library(sf)
library(here)
```

**Additional info:** The [Geocomputation with R](https://geocompr.robinlovelace.net/) textbook (Lovelace, Nowosad, and Muenchow, 2019) and the [R sf package](https://r-spatial.github.io/sf/) webpage.

------------------------------------------------------------------------

### B. Understanding spatial df content

*Summary of information contained within a spatial dataframe*

-   read in a spatial dataframe
-   data source: <https://hub.arcgis.com/maps/CalEMA::california-wastewater-treatment-facilities/about>

CONSIDER INCLUDING LAYERS

```{r}
# Read in the shapefile
?st_read
#dsn= data source name

#shp = one of the main geospatial file types (), part of a dataset
                              

schools_sf = st_read(dsn =here("data",
                               "California_Schools_2019-20",
                               "SchoolSites1920.shp"))
#data source: https://gis.data.ca.gov/datasets/f7f818b0aa7a415192eaf66f192bc9cc/explore 
```

Output ; metadata. we're gonna go over most of what is contained bit b bit throughout the lesson. To make to make it relatable, let's first see how the data we loaded is similar to a base R dataframe

-   ~~driver: ESRI Shapefile; need folder to contain . (MAY NOT BE NEEDED)~~

-   

------------------------------------------------------------------------

### C. Understanding congruencies between base R and spatial dataframes

*Summary of how* *working with a spatial dataframes is similar to working with a base R dataframe*

```{r}

class(schools_sf) #get a sense of the data type 
View(schools_sf) # see that it's similar to base R dataframe 
dim(schools_sf) #get a sense of the size

```

-   when referencing geospatial data

    -   features = rows

    -   attributes = columns containing data

    -   each feature has associated geometry column

```{r}
colnames(schools_sf) #understand the contents
```

All spatial data (vector) has an associated geometry column. This column is fundamental to the definition of an sf dataframe.

What is the geometry of a spatial dataframe?

### D. ... geometry types

*Summary on the different ways spatial data can be formatted (e.g. what are the different spatial geometries)*

~~(maybe define Well Known Text - why is this important though\*\*)~~

points: individual XY locations (e.g. locations of plants (power, water etc), landmarks)

lines: two connected points (e.g. roads)

polygons: three+ connected, closed points (county boundaries, bodies of water)

JUSTFYING the use of multi -geomtries

\*

![](https://raw.githubusercontent.com/NEONScience/NEON-Data-Skills/dev-aten/graphics/vector-general/Attribute_Table.png)

and their multi-equivalents e.g. multipolygon - which can account for irregular/complex shapes, disconnected or intersecting boundaries etc.

the geometry column defines the type and location of the geometry

```{r}
st_geometry(schools_sf)
schools_sf$geometry
```

-   point: location of facility

-   

```{r}
#data source: https://oehha.ca.gov/calenviroscreen/report/calenviroscreen-40

cal_enviro <- st_read("data/calenviroscreen40shpf2021shp/CES4 Final Shapefile.shp")
#revisit the use of dsn

cal_enviro$geometry

#or 
st_geometry(cal_enviro)

```

```{r}
counties = st_read(dsn = here("data",  #dsn= data source name
                              "california_counties", 
                              "CaliforniaCounties.shp"))

counties$geometry
#or
st_geometry(counties)
```

```         
```

-   not so clear what the polygon geometry is surrounding - the information is often contained in the metadata/source of data file

```{r}
View(counties)
```

-   

-   in this example the geometries pertain to the California counties

-   polygon = county boundaries

    -   justify multi-geometry

```{r}
# Maybe notes to include
# Geometries: how do we know what we are plotting?

# know your data
# 
# check attributes: here we have a "NAME" column that tells us CA counties, so we know the boundaries are CA county boundaries. These boundaries are encoded in the geometry column
# 
# location: geometry column
# 
# attribute/data:
# 
# Geometry Types
# 
# Question or challenge: could be around developing an X Y plot of the same data that we plot in section 5: and this is what is used to get at the essential question of what additional insight a spatial dimension gives
# 
# Working with a spatial dataframes is similar to working with a base R dataframe because

```

Other meta-data output by st_geometry

-   dimension: XY - we're dealing with 2-D objects.

    -   we can have 3D: XYZ where Z is elevation

-   projected CRS

------------------------------------------------------------------------

# **2.3 Coordinate Reference Systems**

[**Objective: Navigate different coordinate reference systems (CRS)**]{.underline}

*Intro to section*

-   Define a CRS

-   Why are CRS' important or why are there so many

------------------------------------------------------------------------

### A. Geographic vs. projected CRS

*Example of types of coordinate reference systems (CRS) to show that it has to be specific to the application*

-   **Geographic CRS:**

    -   Uses latitude and longitude to specify locations on the Earth's surface.

    -   Represents a 2D coordinate system.

    -   Suitable for global reference and navigation.

    -   Angular units (degrees) are used.

    -   Geodetic datum (e.g., WGS84) defines the shape of the Earth's surface.

-   **Projected CRS:**

    -   Utilizes x and y coordinates (e.g., meters or feet) on a flat surface for mapping.

    -   Represents a 2D or 3D coordinate system.

    -   Suitable for accurate mapping, spatial analysis, and cartography.

    -   Linear units (meters, feet) are used.

    -   Provides accurate measurements of distances, areas, and angles on flat maps.

    -   Involves mathematical transformations to flatten the Earth's curved surface onto a map plane.

    -   Various projection methods are used to preserve specific properties, such as area, shape, distance, or direction, depending on the application requirements.

![](https://www.earthdatascience.org/images/earth-analytics/spatial-data/spatial-projection-transformations-crs.png)

Global applications benefit from geographic CRS like WGS84, while regional analyses, such as census data interpretation, often use localized geographic CRS like NAD83.

Projected CRS, such as Web Mercator, are ideal for online maps, while state-specific CRS like CA Albers Equal Area cater to localized precision in areas like California.

```{r}
st_geometry(schools_sf)
```

```{r}
st_geometry(counties)
```

output was

-   Geodetic CRS: WGS 84 : a three-dimensional coordinate system (latitude, longitude, and ellipsoidal height). Accounts for earths curvature and shape- represented by an ellipsoid.

    -   \*\*\* may then need to note why we're in XY here

-   "Projected CRS: NAD83 / California Albers"

may need to include

-   *`geocentric` latitude and longitude assume a spherical (round) model of the shape of the earth*

-   *`geodetic` latitude and longitude assume a spheriodal (ellipsoidal) model, which is closer to the true shape.*

Notes

All map projections introduce distortion in area, shape, distance or direction. Specific map projections minimize distortion in one or more properties\

You need to know the coordinate reference system of your input data

You need to select the PCS that is most suitable for your data and application.

\*?include EPSG codes here?

Transition: Why is knowing this detail about the CRS important?

------------------------------------------------------------------------

### B. Briefing on CRS codes

*Summary of how CRS are encoded & extracting CRS codes*

What do you need to know when working with CRS'

-   What CRSs are used in your study area and their main characteristics

-   How to identify, or `get`, the CRS of a spatial dataframe

-   How to `set` the CRS of spatial dataframe (i.e. define the projection)

-   How to reproject CRS for data integration

to ensure accurate interpretation of data

```{r}
st_crs(schools_sf)
```

------------------------------------------------------------------------

### C. CRS Reprojections

*Summary on CRS transformations (needs work)*

-   cautions on reprojecting (?) may or may not include

```{r}
#First, get CRS you want to use
st_crs(counties)

#then input ...
schools_sf_new_CRS =st_transform(schools_sf, crs= st_crs(counties))
```

CRS transformatios (needs work)

-   ...

-   ...

## creating a Spatial dataframe (may not keep)

```{r}
alameda_schools_df <- read.csv(here("data",
                            "alco_schools.csv"))

head(schools_df)
```

```{r}
alameda_schools_sf <- st_as_sf(schools_df, 
                       coords = c('X','Y'),
                       crs = 4326)
```

```{r}
# Save to shapefile, deleting existing file if present
st_write(alameda_schools_sf, 
         here("data", #stating where we want to save the data
              "alameda_schools.shp"),
         delete_dsn = T) #this allows us to overwrite the existing alameda_county files 

#note the .dbf, .prj, .shx files that are also created
```

# 2.2 Basic plotting (&)

To visualize what the geometry means :

```{r}
plot(schools_sf$geometry,col = 'purple')
```

```{r}
plot(counties$geometry) 

```

[**Objective:**]{.underline}

### B. Overlay Plotting

*summary of the concept of overlay*

-   ...

```{r}

#plot old CRS'
plot(counties$geometry, col ="lightgrey",border = 'grey')
plot(schools_sf$geometry,col = 'purple', add = T)

#overlaying plots, border and fill colors etc. 
```

QUESTION: What may have gone wrong?

```{r}
#plot transformed CRS
plot(counties$geometry, col ="lightgrey",border = 'grey')
plot(schools_sf_new_CRS$geometry, col='purple', add=T)

```

Some fundamentals ways to visualize spatial data, and the information contained include

-   ...

### C. Other Plotting Options

```{r}
library(tmap)

# plot a 'quick tmap'
qtm(counties) #see warning

```

-   Sometimes during data creation or processing, polygon geometry gets a bit messed up. It may look great but one or more of the polylines may self-intersect or not close (i.e. snap to a node). This can cause some functions to return an error message or warning. The `tmap_option` **check.and.fix** and repair invalid geometry so that it can render an interactive or static map properly. You can also use the `sf` function **st_make_valid** to repair invalid geometry. See the function documentation for more information.

```{r}
tmap_options(check.and.fix = TRUE)
counties<-st_make_valid(counties)
qtm(counties)
```

```{r}
# toggle the mode (or ttm!)
ttm()
qtm(counties) 
```

More editing options

```{r}
ttm()
tm_shape(counties) +  # use the `tm_shape` function to create a tmap object
  tm_polygons(col = 'tan', # add `tm_polygons` layer, coloring as before
              border.col = 'darkgreen', 
              alpha = 0.5) # & making transparent
```

Overlaying with tmap

-   helps with exploratory analyses

```{r}

tm_shape(counties) +  
  tm_polygons(col = 'tan', 
              border.col = 'darkgreen', 
              alpha = 0.5)+ #note that you use tm_shape first to call the dataset, before later calling what it is. e.g. dots or lines. Just like ggplot
  tm_shape(schools_sf) +
  tm_dots(col = 'purple', 
          border.col = 'white', 
          border.lwd = 1, 
          size = 0.01)

#click into one geometry and see info given


```

## C. Data-Driven Plotting (rename?)

[**(Create thematic maps and understand the information depicted)**]{.underline}

*Intro to section*

-   ...

-   ...

```{r}
library(dplyr)

summary(counties)

#cal_enviro_1<- cal_enviro %>% 
  #filter(across(where(is.numeric), ~. >= 0)) # TOO COMPLICATED! simplify 

summary(counties)  
```

```{r}
library(ggplot2)

#counties$NAME<-as.factor(counties$NAME)

p1 <- ggplot(counties, aes(x = NAME, y = MED_AGE)) +
  geom_col()
p1
```

-   can't make much meaning from this

------------------------------------------------------------------------

### A. Thematic maps

*summary on ways to visualize spatial data and what information is contained in these visualizations*

Chloropleth map:

```{r}

plot(counties['MED_AGE'])
#plot data e.g. med age using XY plot then using
#chloropleth map - to answer 


```

```         
```

-   same data plotted, with the added 'geometry' dimension

## There are three main techniques for improving data visualization:

1.  Color palettes

2.  Data transformations

3.  Classification schemes

## Color Palettes

```{r}
library(ggplot2)

ggplot(counties, aes(fill = MED_AGE)) + 
  geom_sf() +  # tells ggplot that geographic data are being plotted
  #scale_fill_viridis_c() +
  theme_minimal() + 
  labs(title = "Median Age per County")

```

There are three main types of color palettes (or color maps), each of which has a different purpose:

-   **diverging** - a "diverging" set of colors are used so emphasize mid-range values as well as extremes.

-   **sequential** - usually with a single or multi color hue to emphasize differences in order and magnitude, where darker colors typically mean higher values

-   **qualitative** - a contrasting set of colors to identify distinct categories and avoid implying quantitative significance.

![](http://www.gnuplotting.org/figs/colorbrewer.png)</img>

> **Tip**: Sites like [ColorBrewer](https://colorbrewer2.org/#type=sequential&scheme=Blues&n=3) let's you play around with different types of color maps.

To see the names of all color palettes avaialble to `tmap`, try the following command. You may need to enlarge the output image.

```{r}
RColorBrewer::display.brewer.all()
```

#### Diverging

```{r}

library(RColorBrewer)


ggplot(counties, aes(fill = MED_AGE)) + 
  geom_sf() +
  scale_fill_gradientn(colors = brewer.pal(11, "RdBu")) + # Diverging Color Palette
  theme_minimal() + 
  labs(title = "Median Age per County")

```

#### Sequential

```{r}

ggplot(counties, aes(fill = MED_AGE)) + 
  geom_sf() +
  scale_fill_gradientn(colors = brewer.pal(9, "Oranges")) + # Diverging Color Palette
  theme_minimal() + 
  labs(title = "Median Age per County")
```

#### Qualitative & Overlay plotting in ggplot

```{r}
some_counties<- counties %>% filter(MED_AGE<=32)

ggplot() +
  geom_sf(data = some_counties, aes(fill = NAME)) +
  scale_fill_manual(values = brewer.pal(8, "Pastel2")) +
  theme_minimal() +
  labs(title = "Selected Counties")
```

![](http://www.pngall.com/wp-content/uploads/2016/03/Light-Bulb-Free-PNG-Image.png){width="20" align="left"} **Questions**

-   what is lacking with this plot?

```{r}



# Plot counties' geometry and overlay subset_counties with custom colors
overlayed_plot <- ggplot() +
  geom_sf(data = counties, color = "black", fill = "transparent") +
  geom_sf(data = some_counties, aes(fill = NAME)) +
  scale_fill_manual(values = brewer.pal(8, "Pastel2")) +
  theme_minimal() +
  labs(title = "Counties with Median Age < or = 32 Highlighted")


overlayed_plot


```

As a best practice, a `qualitative` color palette should not be used with `quantitative` data and vice versa. For example, consider this map that EDM.com published of top dance tracks by state.

![](https://cdn.vox-cdn.com/thumbor/2AnLp-hwFUEjkW9TxSt_U1-rv8k=/0x0:1198x777/920x0/filters:focal(0x0:1198x777):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/3420378/pandora.0.jpg){width="600px"}</img>

## Notes on Thematic Maps

The goal of a thematic map is to use color to visualize the spatial distribution of a variable in order to identify trends and outliers.

Another goal is to use color to effectively and quickly convey information. For example,

-   maps use brighter or richer colors to signify higher values,

-   and leverage cognitive associations such as mapping water with the color blue.

There are two major challenges when creating thematic maps:

1.  Our eyes are drawn to the color of larger areas or linear features, even if the values of smaller features are more significant.

2.  The range of data values is rarely evenly distributed across all observations and thus the colors can be misleading.

Classification piece - not enough illustration on equal intervals, and quantifiers ... more visualÂ 

-   More examples on data trans and class scheme (not all 5 though). Examples t

-   Eq intervals, quantiles and natural breaks - emphasize those 3

## Transforming Count Data

For a number of reasons, data are often distributed in aggregated form. For example, the Census Bureau collects data from individual people, households and businesses and distributes it aggregated to states, counties, and census tracts, etc.

When the aggregated data are counts, like total population, they can be transformed to densities, proportions and ratios. These normalized variables are more comparable across regions that differ greatly in size.

Let's consider this in terms of our data.

-   **Counts**
    -   data counts, aggregated by feature
        -   *e.g. population within a county*
-   **Densities**
    -   counts aggregated by feature and normalized by feature area
        -   *e.g. population per square mile within a county*
-   **Proportions / Percentages**
    -   value in a specific category divided by total value across in all categories
        -   *e.g. proportion of the county population that is non-white*
-   **Rates / Ratios**
    -   value in one category divided by value in another category, e.g.:
        -   *COVID-19 cases per 100,000 persons*
        -   *COVID-19 R Factor: number of people likely to be infected by one person with COVID-19*

The basic cartographic rule is that when mapping data for areas that differ in size you rarely map counts since those differences in size make the comparison less valid or informative.

### Counts

```{r}
# Map of ...
tm_shape(counties) +
  tm_polygons(col='MULT_RACE', alpha=0.5,
              #palette="Greens",
              title = "mult in 2012")
```

![](http://www.pngall.com/wp-content/uploads/2016/03/Light-Bulb-Free-PNG-Image.png){width="20" align="left"} **Questions**

-   Do you see either of these problems in our density map?

    -   Take a look at the histogram below as you consider the above question.

```{r}
hist(counties$MULT_RACE,
     breaks = 40, 
     main = 'Mult ')
```

### **Proportions / Percentages**

```{r}
counties$MULT_RACE_Prop<-counties$MULT_RACE/counties$POP2012 *100

tm_shape(counties) +
  tm_polygons(col='MULT_RACE_Prop',
              #palette="Greens",
              title = "... ")
```

Consider bringing in CalEnviron data to show how transoformations can be helpful

Notes:

-   `tmap`: for great custom static and interactive maps
-   `mapview`: for a quick and easy interactive map
-   `leaflet`: for highly custom interactive maps that you can output and host on a website
-   `shiny`: for interactive R based applications that use leaflet maps

------------------------------------------------------------------------

# Classification schemes

Another way to make more meaningful maps is to improve the way in which data values are associated with colors.

The common alternative to a proportional color map is to use a **classification scheme** to create a **graduated color map**. This is the standard way to create a **choropleth map**.

A **classification scheme** is a method for binning continuous data values into 4-7 classes (the default is 5) and then associate those classes with the different colors in a color palette.

### The commonly used classifications schemes:

-   **Equal intervals** or **Pretty**
    -   equal-size data ranges (e.g., values within 0-10, 10-20, 20-30, etc.)
    -   <u> pros</u>:
        -   best for data spread across entire range of values
        -   easily understood by map readers
    -   <u>cons</u>:
        -   avoid if you have highly skewed data or a few big outliers because one or more of the bins may have no data observations
-   **Quantiles**
    -   equal number of observations in each bin
    -   <u>pros</u>:
        -   looks nice, because it best spreads colors across full set of data values
        -   thus, it's often the default scheme for mapping software
    -   <u>cons</u>:
        -   bin ranges based on the number of observations, not on the data values
        -   thus, different classes can have very similar or very different values.
-   **Natural breaks**
    -   minimize within-class variance and maximize between-class differences
    -   e.g. 'fisher-jenks',
    -   <u>pros</u>:
        -   great for exploratory data analysis, because it can identify natural groupings
    -   <u>cons</u>:
        -   class breaks are best fit to one dataset, so the same bins can't always be used for multiple years
-   **Head/Tails**
    -   a new relatively new scheme for data with a heavy-tailed distribution
-   **Manual**
    -   classifications are user-defined
    -   <u>pros</u>:
        -   especially useful if you want to slightly change the breaks produced by another scheme
        -   can be used as a fixed set of breaks to compare data over time
    -   <u>cons</u>:
        -   more work involved

### Classification schemes and `tmap`

Classification schemes can be implemented using the `tmap` geometry functions (`tm_polygons`, `tm_dots`, etc.) by setting a value for the **style** argument.

Here are some of the `tmap` keyword names for the different `classification styles` (see the documentation: `?tm_polygons`):

-   `equal`, `quantile`,`fisher`, `jenks`, `headtails`, `fixed`, `kmeans`, `pretty`.

For more information about classification schemes see `?classIntervals` or sources such as [this page](https://geocompr.robinlovelace.net/adv-map.html) in the `Geocomputation with R` ebook.

------------------------------------------------------------------------

#### Classification schemes in action

Let's redo the previous map using the `quantile` classification scheme.

-   What is different about the code? About the output map?

Instance where proportions/percentages doesn't make the data more representative

#### Equal Intervals

```{r}
tmap_mode('plot')
# Plot population density - mile^2
tm_shape(counties) + 
  tm_polygons(col = 'POP12_SQMI',
              #alpha = 0.5,
              title = "Population Density per mi^2")

#default style is pretty(?)
```

#### Quantiles

```{r}
tmap_mode('plot')
# Plot population density - mile^2
tm_shape(counties) + 
  tm_polygons(col = 'POP12_SQMI',
              #alpha = 0.5
              style = "quantile", #style of the break
              title = "Population Density per mi^2") 
```

#### Natural Breaks

```{r}
tmap_mode('plot')
# Plot population density - mile^2
tm_shape(counties) + 
  tm_polygons(col = 'POP12_SQMI',
              style = "fisher", #style of the break
              #alpha = 0.5,
              title = "Population Density per mi^2")
```

-   note the range of each bin

### Manual/User Defined Classification Schemes

You may get pretty close to your final map without being completely satisfied. In this case you can manually define a classification scheme.

Let's customize our map with a `user-defined` classification scheme where we manually set the breaks for the bins using the `classification_kwds` argument.

```{r}
tm_shape(counties) + 
  tm_polygons(col = 'POP12_SQMI',
              palette = "YlGn", 
              style = 'fixed',
              breaks = c(0, 50, 100, 200, 300, 400, max(counties$POP12_SQMI)),
              #labels = c('<50','50 to 100','100 to 200','200 to 300','300 to 400','>400'),
              title = "Population Density per Square Mile")
```

## Summary

### C. Basic Plotting summary

we learned that ..., thus are able to use -- (functions) to ... (\*\* focus on the functions used and not on content, the content focus comes at the end of each subsection)

1.  Fundamentals ways to visualize spatial data include:

    1.  plot
    2.  

------------------------------------------------------------------------

### Challenge 1:

```{r}
TBD
```

------------------------------------------------------------------------

```{r}
st_as_sf #(?)
```

# 

### D. Saving spatial dataframes (and plots (?))

*Summary of different formats that spatial dataframes take on*

-   ...

-   ...

```{r}

st_write #.shp, .json, .gpkg etc.
#differences between shp = ,ultiple files, geojson = 1
#changing extensions
#difference with saving CSV file, using layers_option, and explantion of "Well Known Text" 
#clarify whether the CRS is stored with CSV
```

Spatial dataframes can be saved as

-   different file extensions which ...

------------------------------------------------------------------------

### E. CRS summary

we learned to

1.  the main types of CRS's include
    1.  \*\* mention relation to one another
2.  CRS's are encoded using:
    1.  ...
    2.  st_crs
3.  Data can be formated to fit more suitable CRSs via:
    1.  ...
    2.  st_transform
4.  The formats spatial dataframes take on include:
    1.  ...
    2.  st_write

------------------------------------------------------------------------

### Challenge 2:

```{r}
TBD
```

------------------------------------------------------------------------

### E. Vector data structure summary

\*\* could reformat to be a challenge?

we learned that ..., thus are able to use -- (functions) to ... (\*\* focus on the functions used and not on content, the content focus comes at the end of each subsection)

1.  The types of files we handle when dealing with R spatial dataframes are:

    1.  responds, and new function used

2.  The information contained within a spatial dataframe includes:

    1.  ...
    2.  st_read

3.  Working with a spatial dataframe is similar to working with a base R dataframe because:

    1.  st_as_sf

4.  Different ways spatial data can be formatted/ different spatial geometries include:

5.  Fundamentals ways to visualize spatial data include:

    1.  plot

6.  The formats spatial dataframes take on include:

    1.  ...

    2.  st_write
